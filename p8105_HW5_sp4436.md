p8105_hw5_sp4436
================
Sukhman Parhar
2025-11-15

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.1     ✔ stringr   1.5.2
    ## ✔ ggplot2   4.0.0     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
library(haven)
library(readr)
library(p8105.datasets)
library(ggridges)
library(dplyr)
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'
    ## 
    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

``` r
library(patchwork)
library(broom)

set.seed(1)
```

## Problem 1

``` r
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

bday_sim(20)
```

    ## [1] FALSE

``` r
bday_results = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(group_size, bday_sim)
  ) |> 
  group_by(group_size) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

``` r
bday_results |> 
  ggplot(aes(x = group_size, y = prob_repeat)) + 
  geom_point() + 
  geom_line()
```

![](p8105_HW5_sp4436_files/figure-gfm/Probability%20Plot-1.png)<!-- -->

The probability of observing at least one shared birthday increases
nonlinearly as group size grows. It is nearly zero for small groups
(fewer than 10 people) and it rises quickly afterward. Around 23 people
the probability exceeds 50%, and by 40 people it is above 90%.

## Problem 2

``` r
sim_one_sample = function(n = 30, true_mu = 0, true_sigma = 5) {
  
#Simulate data
  x = rnorm(n, mean = true_mu, sd = true_sigma)
  
#One-sample t-test of H0: mu = 0
  test_result = t.test(x, mu = 0)
  
#Return estimate + p-value
  broom::tidy(test_result) |>
    select(estimate, p.value)
}
```

``` r
set.seed(1)
sim_results =
  expand_grid(
    true_mu = 0:6,
    iter = 1:5000
  ) |>
  mutate(
    output = map(true_mu, ~ sim_one_sample(true_mu = .x))
  ) |>
  unnest(output)

sim_results |>
  select(
    true_mu,
    mu_hat = estimate,
    p_value = p.value
  ) |>
  knitr::kable(digits = 3) |>
  head(10)
```

    ##  [1] "| true_mu| mu_hat| p_value|" "|-------:|------:|-------:|"
    ##  [3] "|       0|  0.412|   0.629|" "|       0|  0.664|   0.368|"
    ##  [5] "|       0|  0.551|   0.534|" "|       0|  0.567|   0.487|"
    ##  [7] "|       0| -1.650|   0.060|" "|       0|  1.185|   0.229|"
    ##  [9] "|       0|  0.334|   0.738|" "|       0| -1.190|   0.209|"

``` r
power_summary = sim_results |>
  mutate(true_mu = as.numeric(true_mu)) |>
  group_by(true_mu) |>
  summarize(power = mean(p.value < 0.05))

power_summary |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line(linewidth = 1.2, color = "blue") +
  geom_point(size = 3, color = "blue") +

  #Type I error reference line
  geom_hline(yintercept = 0.05, linetype = "longdash", color = "red", linewidth = 1.1) +

  #80% power reference line
  geom_hline(yintercept = 0.80, linetype = "longdash", color = "purple", linewidth = 1.1) +

  labs(
    title = "Statistical Power vs. Effect Size",
    subtitle = "Based on 5,000 simulations per group (n = 30)",
    x = "True Value of μ (Effect Size)",
    y = "Proportion of Times H₀ Rejected (Power)"
  ) +
  theme_minimal()
```

![](p8105_HW5_sp4436_files/figure-gfm/Power%20Plots-1.png)<!-- -->

Power increases monotonically as the true mean moves further from 0. As
effect size increases, power also increases. When μ = 0, the rejection
rate is approximately 0.05, matching the expected Type I error rate. As
μ increases, the null hypothesis becomes more false, and the probability
of rejection rises.

``` r
#Average estimate across all simulations
avg_estimate =
  sim_results |>
  group_by(true_mu) |>
  summarize(mean_estimate = mean(estimate))

#Average estimate conditional on rejection
avg_estimate_reject =
  sim_results |>
  mutate(reject = p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(mean_estimate_reject = mean(estimate[reject]))

#Combine both
combined =
  avg_estimate |>
  left_join(avg_estimate_reject, by = "true_mu")

#Overlay
combined |>
  ggplot(aes(x = true_mu)) +
  
  #Unconditional mean (black)
  geom_line(aes(y = mean_estimate),
            color = "black", linewidth = 1.2) +
  geom_point(aes(y = mean_estimate),
             color = "black", size = 3) +
  
  #Conditional-on-rejection mean (purple)
  geom_line(aes(y = mean_estimate_reject),
            color = "purple", linewidth = 1.2) +
  geom_point(aes(y = mean_estimate_reject),
             color = "purple", size = 3) +
  
  labs(
    title = "Average Estimate of μ vs True μ",
    subtitle = "Black: overall mean estimate; Purple: mean estimate among rejections",
    x = "True μ",
    y = "Average estimate of μ̂"
  ) +
  theme_minimal()
```

![](p8105_HW5_sp4436_files/figure-gfm/mu%20plots-1.png)<!-- -->

The black line shows that the sample mean is an unbiased estimator.
across all simulations, the average estimate closely matches the true
value of μ. The purple line showing the average estimate only among
tests that reject the null is biased upward when μ is small. This occurs
because low effect sizes produce low power and conditioning on
significance (p \< 0.05) captures the unusually large positive sample
means that generate rejection. As μ increases and power approaches 100%,
nearly all samples reject the null, causing the conditional (purple) and
unconditional (black) averages to converge toward the true μ.

## Problem 3

``` r
homicide_df = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The raw data has 52179 observations with 12 variables: `uid`,
`reported_date`, `victim_last`, `victim_first`, `victim_race`,
`victim_age`, `victim_sex`, `city`, `state`, `lat`, `lon`,
`disposition`. There are 120 NAs between `lat` and `lon`.

``` r
city_summary =
  homicide_df |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

knitr::kable(
  city_summary,
  caption = "Total and Unsolved Homicides by City",
  col.names = c("City-State", "Total Homicides", "Unsolved Homicides"),
  digits = 0
)
```

| City-State         | Total Homicides | Unsolved Homicides |
|:-------------------|----------------:|-------------------:|
| Albuquerque, NM    |             378 |                146 |
| Atlanta, GA        |             973 |                373 |
| Baltimore, MD      |            2827 |               1825 |
| Baton Rouge, LA    |             424 |                196 |
| Birmingham, AL     |             800 |                347 |
| Boston, MA         |             614 |                310 |
| Buffalo, NY        |             521 |                319 |
| Charlotte, NC      |             687 |                206 |
| Chicago, IL        |            5535 |               4073 |
| Cincinnati, OH     |             694 |                309 |
| Columbus, OH       |            1084 |                575 |
| Dallas, TX         |            1567 |                754 |
| Denver, CO         |             312 |                169 |
| Detroit, MI        |            2519 |               1482 |
| Durham, NC         |             276 |                101 |
| Fort Worth, TX     |             549 |                255 |
| Fresno, CA         |             487 |                169 |
| Houston, TX        |            2942 |               1493 |
| Indianapolis, IN   |            1322 |                594 |
| Jacksonville, FL   |            1168 |                597 |
| Kansas City, MO    |            1190 |                486 |
| Las Vegas, NV      |            1381 |                572 |
| Long Beach, CA     |             378 |                156 |
| Los Angeles, CA    |            2257 |               1106 |
| Louisville, KY     |             576 |                261 |
| Memphis, TN        |            1514 |                483 |
| Miami, FL          |             744 |                450 |
| Milwaukee, wI      |            1115 |                403 |
| Minneapolis, MN    |             366 |                187 |
| Nashville, TN      |             767 |                278 |
| New Orleans, LA    |            1434 |                930 |
| New York, NY       |             627 |                243 |
| Oakland, CA        |             947 |                508 |
| Oklahoma City, OK  |             672 |                326 |
| Omaha, NE          |             409 |                169 |
| Philadelphia, PA   |            3037 |               1360 |
| Phoenix, AZ        |             914 |                504 |
| Pittsburgh, PA     |             631 |                337 |
| Richmond, VA       |             429 |                113 |
| Sacramento, CA     |             376 |                139 |
| San Antonio, TX    |             833 |                357 |
| San Bernardino, CA |             275 |                170 |
| San Diego, CA      |             461 |                175 |
| San Francisco, CA  |             663 |                336 |
| Savannah, GA       |             246 |                115 |
| St. Louis, MO      |            1677 |                905 |
| Stockton, CA       |             444 |                266 |
| Tampa, FL          |             208 |                 95 |
| Tulsa, AL          |               1 |                  0 |
| Tulsa, OK          |             583 |                193 |
| Washington, DC     |            1345 |                589 |

Total and Unsolved Homicides by City

``` r
#Filter for Baltimore
baltimore_data = city_summary |>
  filter(city_state == "Baltimore, MD")

#Run proportion test using total + unsolved
bal_prop_test = with(baltimore_data, {
  prop.test(
    x = unsolved_homicides, 
    n = total_homicides
  )
})

#Tidy output
bal = broom::tidy(bal_prop_test)

bal
```

    ## # A tibble: 1 × 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample… two.sided

``` r
#For each city, run a prop.test using unsolved_homicides and total_homicides
city_tests =
  city_summary |>
  mutate(
    prop_tests = map2(
      unsolved_homicides,
      total_homicides,
      ~ prop.test(x = .x, n = .y)
    ),
    
    tidy_results = map(prop_tests, broom::tidy)
  ) |>
  unnest(tidy_results) |>
  select(city_state, estimate, conf.low, conf.high)
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_tests = map2(...)`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
city_tests
```

    ## # A tibble: 51 × 4
    ##    city_state      estimate conf.low conf.high
    ##    <chr>              <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM    0.386    0.337     0.438
    ##  2 Atlanta, GA        0.383    0.353     0.415
    ##  3 Baltimore, MD      0.646    0.628     0.663
    ##  4 Baton Rouge, LA    0.462    0.414     0.511
    ##  5 Birmingham, AL     0.434    0.399     0.469
    ##  6 Boston, MA         0.505    0.465     0.545
    ##  7 Buffalo, NY        0.612    0.569     0.654
    ##  8 Charlotte, NC      0.300    0.266     0.336
    ##  9 Chicago, IL        0.736    0.724     0.747
    ## 10 Cincinnati, OH     0.445    0.408     0.483
    ## # ℹ 41 more rows

``` r
#Order cities by estimated proportion unsolved
city_tests_ordered =
  city_tests |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))

ggplot(city_tests_ordered, aes(x = city_state, y = estimate)) +
  
  geom_point(color = "#4B0082", size = 3) +
  
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high),
    width = 0.15,
    color = "#4B0082"
  ) +
  
  #Dashed line at 0.50
  geom_hline(
    yintercept = 0.50,
    color = "red",
    linetype = "dashed",
    linewidth = 0.9
  ) +
  
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion Unsolved"
  ) +
  theme_minimal()
```

![](p8105_HW5_sp4436_files/figure-gfm/Plot%20all%20Cities-1.png)<!-- -->
