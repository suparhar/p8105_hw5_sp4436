---
title: "p8105_hw5_sp4436"
author: "Sukhman Parhar"
date: "2025-11-15"
output: github_document
---

```{r Load Packages and set Themes}
library(tidyverse)
library(readxl)
library(haven)
library(readr)
library(p8105.datasets)
library(ggridges)
library(dplyr)
library(janitor)
library(patchwork)
library(broom)

set.seed(1)

```

## Problem 1

```{r Creating Function}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

bday_sim(20)
```

```{r Simulation}
bday_results = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    result = map_lgl(group_size, bday_sim)
  ) |> 
  group_by(group_size) |> 
  summarize(
    prob_repeat = mean(result)
  )
```

```{r Probability Plot}
bday_results |> 
  ggplot(aes(x = group_size, y = prob_repeat)) + 
  geom_point() + 
  geom_line()
```

The probability of observing at least one shared birthday increases nonlinearly as group size grows. It is nearly zero for small groups (fewer than 10 people) and it rises quickly afterward. Around 23 people the probability exceeds 50%, and by 40 people it is above 90%.

## Problem 2

```{r Making Function}
sim_one_sample = function(n = 30, true_mu = 0, true_sigma = 5) {
  
#Simulate data
  x = rnorm(n, mean = true_mu, sd = true_sigma)
  
#One-sample t-test of H0: mu = 0
  test_result = t.test(x, mu = 0)
  
#Return estimate + p-value
  broom::tidy(test_result) |>
    select(estimate, p.value)
}
```

```{r Sim}
set.seed(1)
sim_results =
  expand_grid(
    true_mu = 0:6,
    iter = 1:5000
  ) |>
  mutate(
    output = map(true_mu, ~ sim_one_sample(true_mu = .x))
  ) |>
  unnest(output)

sim_results |>
  select(
    true_mu,
    mu_hat = estimate,
    p_value = p.value
  ) |>
  knitr::kable(digits = 3) |>
  head(10)
```

```{r Power Plots}
power_summary = sim_results |>
  mutate(true_mu = as.numeric(true_mu)) |>
  group_by(true_mu) |>
  summarize(power = mean(p.value < 0.05))

power_summary |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line(linewidth = 1.2, color = "blue") +
  geom_point(size = 3, color = "blue") +

  #Type I error reference line
  geom_hline(yintercept = 0.05, linetype = "longdash", color = "red", linewidth = 1.1) +

  #80% power reference line
  geom_hline(yintercept = 0.80, linetype = "longdash", color = "purple", linewidth = 1.1) +

  labs(
    title = "Statistical Power vs. Effect Size",
    subtitle = "Based on 5,000 simulations per group (n = 30)",
    x = "True Value of μ (Effect Size)",
    y = "Proportion of Times H₀ Rejected (Power)"
  ) +
  theme_minimal()
```

Power increases monotonically as the true mean moves further from 0. As effect size increases, power also increases. When μ = 0, the rejection rate is approximately 0.05, matching the expected Type I error rate. As μ increases, the null hypothesis becomes more false, and the probability of rejection rises.

```{r mu plots}
#Average estimate across all simulations
avg_estimate =
  sim_results |>
  group_by(true_mu) |>
  summarize(mean_estimate = mean(estimate))

#Average estimate conditional on rejection
avg_estimate_reject =
  sim_results |>
  mutate(reject = p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(mean_estimate_reject = mean(estimate[reject]))

#Combine both
combined =
  avg_estimate |>
  left_join(avg_estimate_reject, by = "true_mu")

#Overlay
combined |>
  ggplot(aes(x = true_mu)) +
  
  #Unconditional mean (black)
  geom_line(aes(y = mean_estimate),
            color = "black", linewidth = 1.2) +
  geom_point(aes(y = mean_estimate),
             color = "black", size = 3) +
  
  #Conditional-on-rejection mean (purple)
  geom_line(aes(y = mean_estimate_reject),
            color = "purple", linewidth = 1.2) +
  geom_point(aes(y = mean_estimate_reject),
             color = "purple", size = 3) +
  
  labs(
    title = "Average Estimate of μ vs True μ",
    subtitle = "Black: overall mean estimate; Purple: mean estimate among rejections",
    x = "True μ",
    y = "Average estimate of μ̂"
  ) +
  theme_minimal()
```

The black line shows that the sample mean is an unbiased estimator. across all simulations, the average estimate closely matches the true value of μ. The purple line showing the average estimate only among tests that reject the null is biased upward when μ is small. This occurs because low effect sizes produce low power and conditioning on significance (p < 0.05) captures the unusually large positive sample means that generate rejection. As μ increases and power approaches 100%, nearly all samples reject the null, causing the conditional (purple) and unconditional (black) averages to converge toward the true μ.

## Problem 3

```{r Load and edit data}
homicide_df = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  )
```

The raw data has 52179 observations with 12 variables: `uid`, `reported_date`, `victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`, `city`, `state`, `lat`, `lon`, `disposition`. There are 120 NAs between `lat` and `lon`. 

```{r Murder numbers}
city_summary =
  homicide_df |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = "drop"
  )

knitr::kable(
  city_summary,
  caption = "Total and Unsolved Homicides by City",
  col.names = c("City-State", "Total Homicides", "Unsolved Homicides"),
  digits = 0
)
```

```{r Baltimore ONLY}
#Filter for Baltimore
baltimore_data = city_summary |>
  filter(city_state == "Baltimore, MD")

#Run proportion test using total + unsolved
bal_prop_test = with(baltimore_data, {
  prop.test(
    x = unsolved_homicides, 
    n = total_homicides
  )
})

#Tidy output
bal = broom::tidy(bal_prop_test)

bal
```

```{r}
#For each city, run a prop.test using unsolved_homicides and total_homicides
city_tests =
  city_summary |>
  mutate(
    prop_tests = map2(
      unsolved_homicides,
      total_homicides,
      ~ prop.test(x = .x, n = .y)
    ),
    
    tidy_results = map(prop_tests, broom::tidy)
  ) |>
  unnest(tidy_results) |>
  select(city_state, estimate, conf.low, conf.high)

city_tests
```

```{r Plot all Cities}
#Order cities by estimated proportion unsolved
city_tests_ordered =
  city_tests |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))

ggplot(city_tests_ordered, aes(x = city_state, y = estimate)) +
  
  geom_point(color = "#4B0082", size = 3) +
  
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high),
    width = 0.15,
    color = "#4B0082"
  ) +
  
  #Dashed line at 0.50
  geom_hline(
    yintercept = 0.50,
    color = "red",
    linetype = "dashed",
    linewidth = 0.9
  ) +
  
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion Unsolved"
  ) +
  theme_minimal()
```

